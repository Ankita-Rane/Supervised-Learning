{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition:\n",
    "\n",
    "A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "Ad = pd.read_csv('Advertisement.csv')\n",
    "X = Ad.iloc[:, [2, 3]].values\n",
    "y = Ad.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C - The Regularization parameter (often termed as C parameter in python’s sklearn library) \n",
    "# tells the SVM optimization how much you want to avoid misclassifying each training example.\n",
    "\n",
    "# For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane \n",
    "# does a better job of getting all the training points classified correctly. Conversely, a very small \n",
    "# value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that \n",
    "# hyperplane misclassifies more points.\n",
    "\n",
    "#_________________________________________\n",
    "\n",
    "# Kernel : In machine learning, the radial basis function kernel, or RBF kernel, \n",
    "# is a popular kernel function used in various kernelized learning algorithms. \n",
    "# In particular, it is commonly used in support vector machine classification.\n",
    "\n",
    "#_________________________________________\n",
    "\n",
    "# Learn more about this from:\n",
    "\n",
    "#  https://chrisalbon.com/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma\n",
    "gamma is a parameter of the RBF kernel and can be thought of as the ‘spread’ of the kernel and therefore \n",
    "the decision region. **When gamma is low, the ‘curve’ of the decision boundary is very low and thus the decision \n",
    "region is very broad. When gamma is high, the ‘curve’ of the decision boundary is high**, which creates islands of\n",
    "decision-boundaries around data points. We will see this very clearly below.\n",
    "\n",
    "#### C\n",
    "C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias, low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66,  2],\n",
       "       [ 8, 24]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
